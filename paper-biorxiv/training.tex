%\documentclass{amsart}
\documentclass{article}
\usepackage{amsmath}
\usepackage{fullpage}
\usepackage{bm}

% fix matrix environment to allow vertical bars
% Ref: http://tex.stackexchange.com/a/2244
\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

\newcommand{\pstay}{\ensuremath{p_\mathrm{stay}}}
\newcommand{\pskip}{\ensuremath{p_\mathrm{skip}}}
\newcommand{\pskipone}{\ensuremath{p_\mathrm{skip1}}}
\newcommand{\pstep}{\ensuremath{p_\mathrm{step}}}
\newcommand{\suffix}{\ensuremath{\mathrm{suffix}}}
\newcommand{\prefix}{\ensuremath{\mathrm{prefix}}}
\newcommand{\fw}{\ensuremath{\mathrm{fw}}}
\newcommand{\bw}{\ensuremath{\mathrm{bw}}}
\newcommand{\posterior}{\ensuremath{\mathrm{posterior}}}

\begin{document}

\title{Nanocall Training}
\author{Matei David, Jonathan Dursi, Delia Yao, Paul Boutros, and Jared Simpson}
\date{\today}

\maketitle
\begin{abstract}
This document describes the updates to the pore model scaling parameters and the state transition parameters computed by Nanocall in one training round.
\end{abstract}

\section{Introduction}

When training is enabled, Nanocall performs several training rounds that update both the pore model scaling parameters (\emph{shift}, \emph{scale}, \emph{drift}, \emph{var}, \emph{scale\_sd}, \emph{var\_sd}) and the state transition parameters (\pstay, \pskip). Training stops either when a maximum number of rounds is reached, or when the improvement observed from the previous round drops below a certain threshold. These options are exposed as command line parameters.

Nanocall can use either two sets (one per strand) or a single common set of pore model scaling parameters, but it will always use per-strand state transition parameters. During each round, Nanocall processes 2 event subsequences from the start and end of either one or both strands. First, Nanocall runs the Forward-Backward algorithm separately on each subsequence using the current parameters. Next, Nanocall updates the pore model scaling parameters as described in Section~\ref{scaling-sec}. Finally, Nanocall updates the transition parameters as described in Section~\ref{transition-sec}.

\section{Notation}

We use the following notation:

\begin{description}
\item[$n$] Number of events.
\item[$i$] Index over events, so $i \in \{1, \dots, n\}$.
\item[$m$] Number of states, so $m = 4^6$ if using 6-mers.
\item[$j$] Index over states, so $j \in \{1, \dots, m\}$.
\item[$x_i$] Observed mean for event $i$.
\item[$y_i$] Observed stdv for event $i$.
\item[$t_i$] Measurement time for event $i$.
\item[$\mu_j$] Model level mean for state $j$.
\item[$\sigma_j$] Model level stdv for state $j$.
\item[$\eta_j$] Model spread mean for state $j$.
\item[$\gamma_j$] Model spread stdv for state $j$.
\item[$\lambda_j$] Model spread (IG) shape for state $j$.
\item[$(a,b,c,d,v,u)$] Scaling parameters to compute, corresponding to, in order: \emph{shift}, \emph{scale}, \emph{drift}, \emph{var}$^2$, \emph{scale\_sd}, and \emph{var\_sd}.
\item[$p_{i,j}$] Probability event $i$ was emitted from state $j$.
\item[$f_{i,j}$] Gaussian pdf for emitting event level $i$ from state $j$.
\item[$g_{i,j}$] Inverse Gaussian pdf for emitting event stdv $i$ from state $j$.
\item[$(A,B)$] For matrices $A$ and $B$ with the same number of rows, $(A,B)$ is the matrix containing $A$ on the left and $B$ on the right.
\item[$(A;B)$] For matrices $A$ and $B$ with the same number of columns, $(A;B)$ is the matrix containing $A$ on top and $B$ on the bottom.
\item[$A^{(k)}$] Is the column $(A;\dots;A;\dots;A)$ with $A$ repeated $k$ times.
\item[$x$] The column vector of $x_i$.
\item[$t$] The column vector of $t_i$.
\item[$\suffix(k,i)$] The last $i$ bases in kmer $k$.
\item[$\prefix(k,i)$] The first $i$ bases in kmer $k$.
\end{description}

\section{Pore Model Scaling Parameters}
\label{scaling-sec}

The training procedure detailed in this section was originally described in the ONT internal document~\cite{ont-calibration}. Our purpose here is first to clarify some of the original notation, and second to provide more details for the quantities computed in the Nanocall source code.

\subsection{Gaussian Parameters}

We want to compute the Gaussian scaling parameters $(a,b,c,d)$ that maximize the log likelihood of the observed measurements:
\begin{align*}
\log f_e & = \sum_{i,j} p_{i,j} \log f_{i,j}
\\
& = \sum_{i,j} p_{i,j} \left( - \frac{(x_i - \mu'_{i,j})^2}{2 \sigma'^2_j} - \frac{\log 2 \pi \sigma'^2_j}{2} \right)
\\
& = - \left( \sum_{i,j} \frac{p_{i,j} \left( x_i - a - b \mu_j - c t_i \right)^2}{2 d \sigma^2_j} \right)
- \left( \sum_{i,j} \frac{p_{i,j} \log 2 \pi d \sigma^2_j}{2} \right)
\\
& = - \frac{1}{2d} \left( \sum_{i,j} w_{i,j} \left( x_i - a - b \mu_j - c t_i \right)^2 \right)
- \frac{\log d}{2} \left( \sum_{i,j} p_{i,j} \right)
- \left( \sum_{i,j} \frac{p_{i,j} \log 2 \pi \sigma^2_j}{2} \right)
\end{align*}
Where
\begin{description}
\item[$\mu'_{i,j}$] $:= a + b \mu_j + c t_i$.
\item[$\sigma'^2_j$] $:= d \sigma^2_j$.
\item[$w_{i,j}$] $:= \frac{p_{i,j}}{\sigma_j^2}$.
\end{description}
Further note that:
\begin{itemize}
\item The third term is independent of the scaling factors, so we can ignore it.
\item Likewise, we can ignore the $1/2$ factor.
\item $\sum_{i,j} p_{i,j} = n$, as $\forall i, \sum_j p_{i,j} = 1$.
\end{itemize}
Thus, we want to solve:
\[
(\hat{a}, \hat{b}, \hat{c}, \hat{d}) = \mathrm{argmax}_{(a,b,c,d)} 
- \frac{1}{d} \left( \sum_{i,j} w_{i,j} \left( x_i - a - b \mu_j - c t_i \right)^2 \right) - n \log d
\]

\subsubsection{First step}

We first solve for $(a,b,c)$ while keeping $d$ constant. Then, our goal is to solve the weighted linear system:
\begin{align*}
(\hat{a}, \hat{b}, \hat{c}) = \mathrm{argmin}_{(a,b,c)} \sum_{(i,j)} w_{i,j} \left( x_i - a - b \mu_j - c t_i \right)^2
\end{align*}
This is an overdetermined system with $nm$ equations (iterated over using the tuple $(i,j)$) and 3 coefficients. We write it in standard (wikipedia) form as follows. Let:
\begin{description}
\item[$(i,j)$] Index over the $nm$ equations. When using the matrix notation below, we let $(i,j)$ iterate over $i$ first, then over $j$. Thus, the order of equations is:
\[
(1,1), \dots, (n,1), \dots, (1,j), \dots, (n,j), \dots, (1,m), \dots, (n,m)
\]

\item[$\bm{y}$] is the $nm \times 1$ column vector $x^{(m)}$.

\item[$\bm{X}$] is the $nm \times 3$ matrix:
\[
\bm{X} := \left( (1^{(n)}, \mu_1^{(n)}, t); \dots ; (1^{(n)}, \mu_j^{(n)}, t) ; \dots ; (1^{(n)}, \mu_m^{(n)}, t) \right)
\]

\item[$\bm{\beta}$] is the $3 \times 1$ column vector $(a;b;c)$.

\item[$\bm{W}$] is the $nm \times nm$ diagonal matrix of weights:
\[
\bm{W} := \mathrm{diag} \left( (w_{1,1}, \dots, w_{n,1}), \dots, (w_{1,j}, \dots, w_{n,j}), \dots, (w_{1,m}, \dots, w_{n,m}) \right)
\]
\end{description}

With the notation above,
\[
\hat{\bm{\beta}} = \mathrm{argmin}_{\bm{\beta}} \left\| \bm{W}^{1/2} (\bm{y} - \bm{X} \bm{\beta}) \right\|^2
\]
This system has the known solution:
\[
\hat{\bm{\beta}} = \left( \bm{X}^T \bm{W} \bm{X} \right)^{-1} \left( \bm{X}^T \bm{W} \bm{y} \right)
\]
To reconcile this solution with the ONT document, let:
\begin{description}
\item[$\bm{X}_j$] is the $j$-th block in $\bm{X}$, of size $n \times 3$:
\[
\bm{X}_j := \left( 1^{(n)}, \mu_j^{(n)}, t \right)
\]

\item[$\bm{W}_j$] is the $j$-th diagonal block in $\bm{W}$, of size $n \times n$:
\[
\bm{W}_j := \mathrm{diag} \left( w_{1,j}, \dots, w_{n,j} \right)
\]
\end{description}
Repeatedly using the fact that $(A,B) \cdot (C;D) = A \cdot C + B \cdot D$, observe that:
\begin{align*}
\bm{X}^T \bm{W} \bm{X} & = \left(\bm{X}_1^T, \dots, \bm{X}_j^T, \dots, \bm{X}_m^T \right)
\mathrm{diag} \left( \bm{W}_1, \dots, \bm{W}_j, \dots, \bm{W}_m \right)
\left(\bm{X}_1; \dots; \bm{X}_j; \dots; \bm{X}_m \right) \\
& = \sum_j \bm{X}_j^T \bm{W}_j \bm{X}_j, \\
& = \sum_j \left( 1^{(n)}, \mu_j^{(n)}, t \right)^T
\mathrm{diag} \left( w_{1,j}, \dots, w_{n,j} \right)
\left( 1^{(n)}, \mu_j^{(n)}, t \right) \\
\bm{X}^T \bm{W} \bm{y} & = \left(\bm{X}_1^T, \dots, \bm{X}_j^T, \dots, \bm{X}_m^T \right)
\mathrm{diag} \left( \bm{W}_1, \dots, \bm{W}_j, \dots, \bm{W}_m \right)
\left( x; \dots; x; \dots; x \right) \\
& = \sum_j \bm{X}_j^T \bm{W}_j x \\
& = \sum_j \left( 1^{(n)}, \mu_j^{(n)}, t \right)^T
\mathrm{diag} \left( w_{1,j}, \dots, w_{n,j} \right)
x
\end{align*}
To work out the solution, let
\begin{description}
\item[$\bm{A}$] denote the $3 \times 3$ matrix $\bm{X}^T \bm{W} \bm{X}$. Observe that $\bm{A}$ is symmetric by definition.

\item[$\bm{B}$] denote the $3 \times 1$ matrix $\bm{X}^T \bm{W} \bm{y}$.
\end{description}

Then, we have:
\begin{align*}
\bm{A}_{1,1} & = \sum_j \sum_i w_{i,j}
& = \sum_i \left( \sum_j \frac{p_{i,j}}{\sigma_j^2} \right)
& = \sum_i s_{i,0}
\\
\bm{A}_{1,2} & = \sum_j \sum_i w_{i,j} \mu_j
& = \sum_i \left( \sum_j \frac{p_{i,j}}{\sigma_j^2} \mu_j \right)
& = \sum_i s_{i,1}
\\
\bm{A}_{1,3} & = \sum_j \sum_i w_{i,j} t_i
& = \sum_i t_i \left( \sum_j \frac{p_{i,j}}{\sigma_j^2} \right)
& = \sum_i t_i s_{i,0}
\\
\bm{A}_{2,1} & = \bm{A}_{1,2}
\\
\bm{A}_{2,2} & = \sum_j \sum_i w_{i,j} \mu_j^2
& = \sum_i \left( \sum_j \frac{p_{i,j}}{\sigma_j^2} \mu_j^2 \right)
& = \sum_i s_{i,2}
\\
\bm{A}_{2,3} & = \sum_j \sum_i w_{i,j} \mu_j t_i
& = \sum_i t_i \left( \sum_j \frac{p_{i,j}}{\sigma_j^2} \mu_j \right)
& = \sum_i t_i s_{i,1}
\\
\bm{A}_{3,1} & = \bm{A}_{1,3}
\\
\bm{A}_{3,2} & = \bm{A}_{2,3}
\\
\bm{A}_{3,3} & = \sum_j \sum_i w_{i,j} t_i^2
& = \sum_i t_i^2 \left( \sum_j \frac{p_{i,j}}{\sigma_j^2} \right)
& = \sum_i t_i^2 s_{i,0}
\\
\bm{B}_1 & = \sum_j \sum_i w_{i,j} x_i
& = \sum_i x_i \left( \sum_j \frac{p_{i,j}}{\sigma_j^2} \right)
& = \sum_i x_i s_{i,0}
\\
\bm{B}_2 & = \sum_j \sum_i w_{i,j} \mu_j x_i
& = \sum_i x_i \left( \sum_j \frac{p_{i,j}}{\sigma_j^2} \mu_j \right)
& = \sum_i x_i s_{i,1}
\\
\bm{B}_3 & = \sum_j \sum_i w_{i,j} t_i x_i
& = \sum_i t_i x_i \left( \sum_j \frac{p_{i,j}}{\sigma_j^2} \right)
& = \sum_i t_i x_i s_{i,0}
\end{align*}
where $s_{i,k} := \sum_j \frac{p_{i,j}}{\sigma_j^2} \mu_j^k$. Note that, in the special case when $p_{i,j} = \delta_{j,j(i)}$ for some $j(i)$, all sums over $j$ can be replaced by single terms.

We solve this system ``by hand'' using Gaussian elimination as follows. We give up in all cases when the system is found to be singular. We know that $A_{1,1} > 0$, so we first compute:
\begin{align*}
\left[ \bm{A}' | \bm{B}' \right] =
\begin{pmatrix}[ccc|c]
\bm{A}_{1,1} &
\bm{A}_{1,2} &
\bm{A}_{1,3} &
\bm{B}_1
\\
0 &
\bm{A}_{2,2} - \bm{A}_{1,2} \left( \bm{A}_{2,1} / \bm{A}_{1,1} \right) &
\bm{A}_{2,3} - \bm{A}_{1,3} \left( \bm{A}_{2,1} / \bm{A}_{1,1} \right) &
\bm{B}_2 - \bm{B}_1 \left( \bm{A}_{2,1} / \bm{A}_{1,1} \right)
\\
0 &
\bm{A}_{3,2} - \bm{A}_{1,2} \left( \bm{A}_{3,1} / \bm{A}_{1,1} \right) &
\bm{A}_{3,3} - \bm{A}_{1,3} \left( \bm{A}_{3,1} / \bm{A}_{1,1} \right) &
\bm{B}_3 - \bm{B}_1 \left( \bm{A}_{3,1} / \bm{A}_{1,1} \right)
\end{pmatrix}
\end{align*}

Case analysis:
\begin{enumerate}

\item $A'_{2,2} \neq 0$. Let:
\begin{align*}
\left[ \bm{A}'' | \bm{B}'' \right] =
\begin{pmatrix}[ccc|c]
\bm{A}'_{1,1} &
\bm{A}'_{1,2} &
\bm{A}'_{1,3} &
\bm{B}'_1
\\
0 &
\bm{A}'_{2,2} &
\bm{A}'_{2,3} &
\bm{B}'_2
\\
0 &
0 &
\bm{A}'_{3,3} - \bm{A}'_{2,3} \left( \bm{A}'_{3,2} / \bm{A}'_{2,2} \right) &
\bm{B}'_3 - \bm{B}'_2 \left( \bm{A}'_{3,2} / \bm{A}'_{2,2} \right)
\end{pmatrix}
\end{align*}
We consider subcases:
\begin{enumerate}
\item $A''_{3,3} \neq 0$. We have a unique solution:
\begin{align*}
\hat{c} & = \bm{B}''_3 / \bm{A}''_{3,3}
\\
\hat{b} & = \left( \bm{B}'_2 - \bm{A}'_{2,3} \hat{c} \right) / \bm{A}'_{2,2}
\\
\hat{a} & = \left( \bm{B}_1 - \bm{A}_{1,2} \hat{b} - \bm{A}_{1,3} \hat{c} \right) / \bm{A}_{1,1}
\end{align*}

\item $A''_{3,3} = 0$. The system is singluar, we give up.
\end{enumerate}

\item $A'_{2,2} = 0$ and $A'_{3,2} \neq 0$. We consider subcases:
\begin{enumerate}
\item $A'_{2,3} \neq 0$. We have a unique solution:
\begin{align*}
\hat{c} & = \bm{B}'_2 / \bm{A}'_{2,3}
\\
\hat{b} & = \left( \bm{B}'_3 - \bm{A}'_{3,3} \hat{c} \right) / \bm{A}'_{3,2}
\\
\hat{a} & = \left( \bm{B}_1 - \bm{A}_{1,2} \hat{b} - \bm{A}_{1,3} \hat{c} \right) / \bm{A}_{1,1}
\end{align*}

\item $A'_{2,3} = 0$. The system is singluar, we give up.
\end{enumerate}

\item $A'_{2,2} = 0$ and $A'_{3,2} = 0$. The system is singluar, we give up.
\end{enumerate}

\subsubsection{Second Step}

In the previous section, we computed $(\hat{a}, \hat{b}, \hat{c})$ that maximize $- \sum_{i,j} w_{i,j} \left( x_i - a - b \mu_j - c t_i \right)^2$. Let $\alpha := \sum_{i,j} w_{i,j} \left( x_i - \hat{a} - \hat{b} \mu_j - \hat{c} t_i \right)^2$. Now, we want to compute
\[
\hat{d} = \mathrm{argmax}_d - \frac{\alpha}{d} - n \log d
\]
By taking the derivative with respect to $d$ and setting it to 0, we obtain:
\begin{align*}
\hat{d} & = \frac{\alpha}{n} = \frac{\sum_{i,j} w_{i,j} \left( x_i - \hat{a} - \hat{b} \mu_j - \hat{c} t_i \right)^2}{n}
\\
& = \frac{1}{n} \sum_{i,j} w_{i,j} \left(
x^2_i
+ \hat{a}^2
+ \hat{b}^2 \mu^2_j
+ \hat{c}^2 t^2_i
- 2 \hat{a} x_i
- 2 \hat{b} x_i \mu_j
- 2 \hat{c} t_i x_i
+ 2 \hat{a} \hat{b} \mu_j
+ 2 \hat{a} \hat{c} t_i
+ 2 \hat{b} \hat{c} t_i \mu_j
\right)
\\
& = \frac{1}{n} \left(
\left( \sum_i x_i^2 s_{i,0} \right)
+ \hat{a}^2 \sum_i s_{i,0}
+ \hat{b}^2 \sum_i s_{i,2}
+ \hat{c}^2 \sum_i t^2_i s_{i,0}
\right.
\\
& \qquad
- 2 \hat{a} \sum_i x_i s_{i,0}
- 2 \hat{b} \sum_i x_i s_{i,1}
- 2 \hat{c} \sum_i x_i t_i s_{i,0}
\\
& \qquad
\left.
+ 2 \hat{a} \hat{b} \sum_i s_{i,1}
+ 2 \hat{a} \hat{c} \sum_i t_i s_{i,0}
+ 2 \hat{b} \hat{c} \sum_i t_i s_{i,1}
\right)
\\
& = \frac{1}{n} \left(
\left( \sum_i x_i^2 s_{i,0} \right)
+ \hat{a}^2 A_{1,1}
+ \hat{b}^2 A_{2,2}
+ \hat{c}^2 A_{3,3}
\right.
\\
& \qquad
- 2 \hat{a} B_1
- 2 \hat{b} B_2
- 2 \hat{c} B_3
\\
& \qquad
\left.
+ 2 \hat{a} \hat{b} A_{1,2}
+ 2 \hat{a} \hat{c} A_{1,3}
+ 2 \hat{b} \hat{c} A_{2,3}
\right)
\end{align*}

\subsection{Inverse Gaussian Parameters}

To model the event stdv-s, we wany to compute the scaling parameters $(v,u)$ that maximize the log likelihood of the observed measurements:
\begin{align*}
\log g_e & = \sum_{i,j} p_{i,j} \log g_{i,j}
\\
& = \frac{1}{2} \sum_{i,j} p_{i,j} \left( \log \lambda'_j - \log 2 \pi - 3 \log y_i
- \frac{\lambda'_j (y_i - \eta'_j)^2}{y_i \eta'^2_j } \right)
\\
& = \frac{1}{2} \sum_{i,j} p_{i,j} \left( \log u \lambda_j - \log 2 \pi - 3 \log y_i
- \frac{u \lambda_j (y_i - v \eta_j)^2}{y_i v^2 \eta^2_j} \right).
\end{align*}
Where:
\begin{description}
\item[$\lambda_j$] $:= \frac{\eta^3_j}{\gamma^2_j}$.
\item[$\eta'_j$] $:= v \eta_j$.
\item[$\lambda'_j$] $:= u \lambda_j$.
\end{description}

We first update $v$ by solving $\frac{\partial \log g_e}{\partial v} (u, v) = 0$:
\begin{align*}
\frac{\partial \log g_e}{\partial v} (u, v) & = \frac{1}{2} \sum_{i,j} - p_{i,j} \frac{u \lambda_j}{y_i \eta^2_j} \cdot 2 \left( \frac{y_i}{v} - \eta_j \right) \cdot \frac{-y_i}{v^2}
\\
& = \frac{u}{v^3} \sum_{i,j} p_{i,j} \lambda_j \frac{y_i - v \eta_j}{\eta^2_j}
\\
& = \frac{u}{v^3} \sum_{i,j} p_{i,j} \lambda_j \left(\frac{y_i}{\eta^2_j} - v \frac{1}{\eta_j} \right).
\end{align*}
Setting this to 0, we obtain:
\begin{align*}
\hat{v} & = \frac{\sum_{i,j} p_{i,j} \lambda_j y_i / \eta^2_j}{\sum_{i,j} p_{i,j} \lambda_j / \eta_j}
\\
& = \frac{ \sum_i y_i \sum_j p_{i,j} \lambda_j / \eta^2_j }{ \sum_i \sum_j p_{i,j} \lambda_j / \eta_j }
\end{align*}

Next, we compute:
\begin{align*}
\frac{\partial \log g_e}{\partial u} (u, v) & =
\frac{1}{2} \sum_{i,j} p_{i,j} \left( \frac{1}{u} - \frac{\lambda_j (y_i - v \eta_j)^2}{y_i v^2 \eta^2_j} \right).
\end{align*}
We plug in $\hat{v}$, and update $u$ by solving $\frac{\partial \log g_e}{\partial v} (u, \hat{v}) = 0$:
\begin{align*}
\frac{1}{\hat{u}} \left( \sum_{i,j} p_{i,j} \right) & =
\sum_{i,j} p_{i,j} \lambda_j \left( \frac{y_i}{\hat{v}^2 \eta_j^2} - \frac{2}{\hat{v} \eta_j} + \frac{1}{y_i} \right)
\\
& = \sum_{i,j} p_{i,j} \lambda_j \left( \frac{1}{y_i} - \frac{1}{\hat{v} \eta_j} \right)
\\
& = \left( \sum_i \frac{1}{y_i} \sum_j p_{i,j} \lambda_j \right)
- \frac{1}{\hat{v}} \left( \sum_i \sum_j p_{i,j} \lambda_j \frac{1}{\eta_j} \right),
\end{align*}
where in the second last step we used the fact that $\sum_{i,j} p_{i,j} \lambda_j y_i / \eta_j^2 = \hat{v} \sum_{i,j} p_{i,j} \lambda_j / \eta_j$. Thus,
\[
\hat{u} = \frac{n}{\left( \sum_i \frac{1}{y_i} \sum_j p_{i,j} \lambda_j \right)
- \frac{1}{\hat{v}} \left( \sum_i \sum_j p_{i,j} \lambda_j \frac{1}{\eta_j} \right)}.
\]

\section{State Transition Parameters}
\label{transition-sec}

Given kmers $k_1$, $k_2$, and state transition parameters \pstay, \pskip, the probability of the transition from $k_1$ to $k_2$ is:
\begin{align*}
\mathrm{tr}(k_1, k_2) = \quad &
\delta_{k_1 = k_2} \cdot \pstay
\\
& + \delta_{\suffix(k_1, 5) = \prefix(k_2, 5)} \cdot \pstep \cdot \frac{1}{4}
\\
& + \sum_{i=2}^5 \delta_{\suffix(k_1, 6-i) = \prefix(k_2, 6-i)} \cdot \pskipone^{i-1} \cdot \frac{1}{4^i}
\\
& + \sum_{i>5} \pskipone^{i-1} \cdot \frac{1}{4^6},
\end{align*}
where:
\begin{description}
\item[$\delta_E$] is the indicator function for event $E$.
\item[$\pstep$] $:= 1 - \pstay - \pskip$.
\item[$\pskipone$] $:= \pskip / (1 + \pskip)$ is the solution of $\sum_{i \ge 1} \pskipone = \pskip$.
\end{description}

To train the state transtion parameters, we first identify those states/kmers $k$ for which there is as little ambiguity as possible for transitioning from $k$ either back to $k$ or to one of its 4 immediate neighbours. Formally, define:
\begin{align*}
\mathrm{dist}(k_1, k_2) & := \min \left\{ i \; | \; \suffix(k_1, 6-i) = \prefix(k_2, 6-i) \right\}
\\
\mathrm{neigh}(k) & := \left\{ k' \; | \; \mathrm{dist}(k,k') = 1 \right\}
\\
\mathrm{selfoverlap}(k) & := \max \left\{ i \; | \; \suffix(k, i) = \prefix(k, i) \right\}.
\end{align*}
With this notation, let:
\[
\mathcal{K} := \left\{ k \; | \; \mathrm{selfoverlap}(k) = 0 \; \mathrm{and} \;
\forall k' \in \mathrm{neigh}(k), \mathrm{selfoverlap}(k') \le 1 \right\}.
\]
Observe that for $\forall k \in \mathcal{K}$, and $\forall k' \in \mathrm{neigh}(k)$,
\begin{align*}
\mathrm{tr}(k, k) & = \pstay + \sum_{i>5} \pskipone^{i-1} \cdot \frac{1}{4^6} \sim \pstay
\\
\mathrm{tr}(k, k') & = \pstep \cdot \frac{1}{4} + \sum_{i>5} \pskipone^{i-1} \cdot \frac{1}{4^6} \sim \pstep \cdot \frac{1}{4}.
\end{align*}
We use the standard Forward-Backward notation:
\begin{align*}
\fw(i, k) & := \Pr [ X_1 = x_i, \dots, X_i = x_i, S_i = k \; | \; \theta ]
\\
\bw(i, k) & := \Pr [ X_{i+1} = x_{i+1}, \dots, X_n = x_n \; | \; S_i = k, \theta ]
\\
\Pr [ X | \theta ] & := \sum_k \fw(n, k)
\\
\posterior(i, k) & := \Pr [ S_i = k | X, \theta ]
= \frac{\fw(i,k) \cdot \bw(i,k)}{\Pr[X | \theta]}
\end{align*}
Following the Baum-Welch algorithm, let:
\begin{align*}
\mathrm{joint\_prob}(i, k, k')
& := \Pr [ S_i = k, S_{i+1} = k' | X, \theta ]
\\
& = \frac{\fw(i, k) \cdot \mathrm{tr}(k, k') \cdot \mathrm{em}(k', i+1) \cdot \bw(i+1, k')}{\Pr[X | \theta]}
\end{align*}
Now we compute:
\begin{align*}
\mathrm{denom}(i)
& := \Pr [ S_i \in \mathcal{K} | X, \theta ]
= \sum_{k \in \mathcal{K}} \posterior(i, k)
\\
\mathrm{numer\_stay}(i)
& := \Pr [ S_i \in \mathcal{K}, S_{i+1} = S_i | X, \theta ]
= \sum_{k \in \mathcal{K}} \mathrm{joint\_prob}(i, k, k)
\\
\mathrm{numer\_skip}(i)
& := \Pr [ S_i \in \mathcal{K}, S_{i+1} \notin \{S_i\} \cup \mathrm{neigh}(S_i) | X, \theta ]
\\
& = \sum_{k \in \mathcal{K}} \left( 1 - \mathrm{joint\_prob}(i, k, k) - \sum_{k' \in \mathrm{neigh}(k)} \mathrm{joint\_prob}(i, k, k') \right)
\end{align*}
Using these quantities, we can compute the following transition parameter updates:
\begin{align*}
\pstay' & := \frac{\sum_{i=1}^{n-1} \mathrm{numer\_stay}(i)}{\sum_{i=1}^{n-1} \mathrm{denom}(i)}
\\
\pskip' & := \frac{\sum_{i=1}^{n-1} \mathrm{numer\_skip}(i)}{\sum_{i=1}^{n-1} \mathrm{denom}(i)}
\end{align*}

\begin{thebibliography}{9}

\bibitem{ont-calibration}
  Tim Massingham,
  \emph{Calibrating and training using Expectation Maximization},
  Oxford Nanopore Wiki.

\end{thebibliography}

\end{document}
